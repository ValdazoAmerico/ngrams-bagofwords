{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ngrams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGHkeaZK+0QDaT3J82JFnX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValdazoAmerico/ngrams-bagofwords/blob/main/ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28VD6tFM47od"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl5PCQX35DFr",
        "outputId": "a11d0201-cbbb-470b-96f5-af940e8c7e69"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcCAvhmX5Fdk",
        "outputId": "69e88b85-1449-474c-9a93-45bbf939b7e1"
      },
      "source": [
        "quote = \" Child is the father of a man \"\n",
        "quote_tokens = word_tokenize(quote)\n",
        "print(quote_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Child', 'is', 'the', 'father', 'of', 'a', 'man']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iunx5uPp5W2l",
        "outputId": "21ba734c-f6cb-4fa6-ffe8-19135c613e45"
      },
      "source": [
        "print(len(quote_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM_s7vKh5m5L"
      },
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams\n",
        "\n",
        "string = \"Love is mightier than power, better than philosophy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jongbQk153Q7",
        "outputId": "850f62ed-b94e-4217-fa2c-645438144e22"
      },
      "source": [
        "tokens = word_tokenize(string)\n",
        "string_bigrams = list(nltk.bigrams(tokens))\n",
        "print(string_bigrams)\n",
        "\n",
        "string_trigrams = list(nltk.trigrams(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Love', 'is'), ('is', 'mightier'), ('mightier', 'than'), ('than', 'power'), ('power', ','), (',', 'better'), ('better', 'than'), ('than', 'philosophy')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqIoGPZZ7mKE",
        "outputId": "17c709ab-76df-446e-b513-5854acb2cf08"
      },
      "source": [
        "string_trigrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Love', 'is', 'mightier'),\n",
              " ('is', 'mightier', 'than'),\n",
              " ('mightier', 'than', 'power'),\n",
              " ('than', 'power', ','),\n",
              " ('power', ',', 'better'),\n",
              " (',', 'better', 'than'),\n",
              " ('better', 'than', 'philosophy')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xg_3Xb77ufz",
        "outputId": "5bbda065-9840-4b4e-cb0b-2b72faa50684"
      },
      "source": [
        "string_ngrams = list(nltk.ngrams(tokens, 5))\n",
        "string_ngrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Love', 'is', 'mightier', 'than', 'power'),\n",
              " ('is', 'mightier', 'than', 'power', ','),\n",
              " ('mightier', 'than', 'power', ',', 'better'),\n",
              " ('than', 'power', ',', 'better', 'than'),\n",
              " ('power', ',', 'better', 'than', 'philosophy')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOtoTXFa77xD",
        "outputId": "503fc90c-d82a-48b7-f24e-a72b782732ee"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "quote = \"Success is journey, not a destination\"\n",
        "result = text_to_word_sequence(quote)\n",
        "print(result)\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(quote)\n",
        "\n",
        "print(t.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['success', 'is', 'journey', 'not', 'a', 'destination']\n",
            "OrderedDict([('s', 5), ('u', 2), ('c', 2), ('e', 3), ('i', 3), ('j', 1), ('o', 3), ('r', 1), ('n', 4), ('y', 1), ('t', 3), ('a', 2), ('d', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHRKucQ58gYv",
        "outputId": "2fdaeff7-27ac-4d71-d8be-2028ccce1109"
      },
      "source": [
        "sentence = ['He is John', \"He is American\"]\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentence)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'he': 1, 'is': 2, 'john': 3, 'american': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA3e2JxJ9PLl",
        "outputId": "55b5102f-ed28-422e-e4bb-1936f2ca9798"
      },
      "source": [
        "sentence = ['John is an Engineer', \"Sam is a doctor\", \"William is not an engineer\"]\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentence)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'is': 1, 'an': 2, 'engineer': 3, 'john': 4, 'sam': 5, 'a': 6, 'doctor': 7, 'william': 8, 'not': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q40IAHfl9YyS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}